---
title: "001_SPL_clean"
output: html_document
date: "2024-04-16"
---

This script creates two new intermediary datasets:

1) SPL_1823.csv: the SPL staying dataset filtered for year 2018 and 2023 with new columns for location_neighborhood (the neighborhood of the observation location, based on the SPL location naming) and S_HOOD (the neighborhood of the observation location, based on the Seattle Neighborhood Map Atlas naming)

2) SPL_1823_location.geojson: the shapefiles and centroid lat/lon for all the location_ids in SPL 2018 and 2023

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load required packages
```{r}
rm(list=ls())
library(tidyverse)
library(psych)
library(dplyr)
library(sf)
library(here)
library(leaflet)

set_here(path='..')
```

# Import all datasets
```{r}
SPL <- read.csv(here("Data/Raw","03_SPL_Staying.csv")) # main dataset with SPL staying observations
SPL_sites <- read.csv(here("Data/Raw","02_SPL_location.csv")) # SPL location_id info
geo_SPL_locationids <- st_read(here("Data/Raw", "01_SPL_Geography.geojson")) # shapefile data with location_id polygons
```

# Subset dataset for 2018 and 2023 data
```{r}
#Create a separate dataset for 2018 and 2023 data
SPL_1823 <- SPL %>%
  filter(study_id=="2018_Seattle_Citywide" | study_id =="2023_Seattle_Citywide")
```

# Create neighborhood column
```{r}
# First match observation site (location_id) with the neighborhoods they're in (location_neighborhood)
SPL_sites <- SPL_sites %>% select(location_id, location_neighborhood) # just select the location_neighborhood col that we care about
SPL_1823 <- left_join(SPL_1823, SPL_sites, by = "location_id") # merge the SPL data with the location_neighborhood col

# Check to see that all location_ids have a corresponding location_neighborhood
unique(SPL_1823[is.na(SPL_1823$location_neighborhood), ]$location_id) # missing location_neighborhood for a few

# Fill in NA location_neighborhoods manually
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "BLV3")] <- "Bitter Lake Village"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "PIK6")] <- "Pike/Pine"
SPL_1823$location_neighborhood[which(SPL_1823$location_id %in% c("PIO20", "PIO21"))] <- "Pioneer Square"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "CAP5")] <- "Capitol Hill"
SPL_1823$location_neighborhood[which(SPL_1823$location_id %in% c("COM5", "COM6", "COM7", "COM8"))] <- "Commercial Core"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "FHT5")] <- "First Hill / 12th Ave"

# Rename neighborhoods to match the names in the geospatial data (S_HOOD)
SPL_1823 <- SPL_1823 %>%
  mutate(S_HOOD = recode(location_neighborhood,
                          "23rd & Union - Jackson" = "Pinehurst",
                          "Capitol Hill" = "Broadway",
                          "Bitter Lake Village" = "Bitter Lake",
                          "Magnolia" = "Carleton Park",
                          "Upper Queen Anne" = "North Queen Anne",
                          "Pike/Pine" = "Pike-Market",
                          "Greenwood - Phinney Ridge" = "Greenwood",
                          "Westwood - Highland Park"= "Highland Park",
                          "Othello" = "Columbia City",                
                          "Madison Miller" = "Stevens",
                          "Uptown" = "Lower Queen Anne",
                          "First Hill / 12th Ave" = "Broadview",
                          "West Seattle Junction" = "Gatewood",
                          "Commercial Core" = "Central Business District",
                          "Mt Baker"= "Mount Baker",              
                          "Chinatown / ID" = "International District",          
                          "Northgate" = "Maple Leaf",              
                          "Lake City" = "Victory Heights"))
```

# Subset geo data for 2018 and 2023 locations
```{r}
# First check to see if we have shapefiles for all location_ids
spl_locids <- unique(SPL_1823$location_id) #122
geo_locids <- unique(geo_SPL_locationids$location_id) #127
spl_locids[which(!spl_locids %in% geo_locids)] # missing shapefiles for these: BLT5, PIK5, BLV3, PIK6, HPT1, PIO21, PIO20, HPT2, CAP5, COM5, COM6, COM7, COM8, FHT5
missingshapes_locids <- spl_locids[which(!spl_locids %in% geo_locids)]

# Identify which study the missing shapefiles are from
SPL_1823 %>% select(study_id, location_id) %>% filter(location_id %in% missingshapes_locids) %>% table() # missing shapefiles are all from 2023 survey

# Filter geodata for just the location_ids in our 2018/2023 data
geo_SPL_1823_locationids <- geo_SPL_locationids %>% filter(location_id %in% spl_locids) # 108 location_ids that are shared between 18 and 23

# Extract lat/long coordinates from centroids
geo_SPL_1823_locationids <- geo_SPL_1823_locationids %>%
  mutate(longitude=st_coordinates(st_centroid(geometry))[,1], #Extract the longitude 
         latitude=st_coordinates(st_centroid(geometry))[,2])   #Extract the latitude

# test plot
ggplot(geo_SPL_1823_locationids) +
  geom_sf()
```

# Add lat/long coordinates for the 2023 location_ids without shapefiles
```{r}
# Get the lat/long coordinates from the main SPL dataset
SPL_1823 %>%
  select(location_id, latitude, longitude) %>%
  filter(location_id %in% missingshapes_locids) %>%
  distinct() # note that the coordinates for same location_id are all slightly different

# just keep the lat/long from the first location_id, assuming all the coordinates are quite similar
latlong_missingshapes_locids <- SPL_1823 %>%
  select(location_id, latitude, longitude) %>%
  filter(location_id %in% missingshapes_locids) %>%
  distinct(location_id, .keep_all = TRUE)

# merge with the other location_ids
geo_SPL_1823_locationids <- merge(geo_SPL_1823_locationids, latlong_missingshapes_locids, all = TRUE)

# test plot
ggplot(geo_SPL_1823_locationids) +
  geom_sf() +
  geom_point(data = geo_SPL_1823_locationids, aes(x = longitude, y = latitude), shape = 20, size = 1)
```


# Export data
```{r}
write.csv(SPL_1823, here("Data", "SPL_1823.csv"))
sf::st_write(geo_SPL_1823_locationids,here("Data", "SPL_1823_location.geojson")) #delete the old file before running this line, otherwise it will say that this dataset already exists and won't override it
```
