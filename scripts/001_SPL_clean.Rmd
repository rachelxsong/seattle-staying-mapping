---
title: "001_SPL_clean"
output: html_document
date: "2024-04-16"
---

This script creates two new intermediary datasets:

1) SPL_1823.csv: the SPL staying dataset filtered for year 2018 and 2023 with new columns for location_neighborhood (the neighborhood of the observation location, based on the SPL location naming) and S_HOOD (the neighborhood of the observation location, based on the Seattle Neighborhood Map Atlas naming)

2) SPL_1823_location.geojson: the shapefiles and centroid lat/lon for all the location_ids in SPL 2018 and 2023

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load required packages
```{r}
rm(list=ls())
library(tidyverse)
library(psych)
library(dplyr)
library(sf)
library(here)
library(leaflet)
```

# Import all datasets
```{r}
SPL <- read.csv(here("data/raw","03_SPL_Staying.csv")) # main dataset with SPL staying observations
SPL_sites <- read.csv(here("data/raw","02_SPL_location.csv")) # SPL location_id info
geo_SPL_locationids <- st_read(here("data/raw", "01_SPL_Geography.geojson")) # shapefile data with location_id polygons

geo_SHOODs <- st_read(here("data/raw", "04_SPL_Seattle_Map.geojson"))

```

# Subset dataset for 2018 and 2023 data
```{r}
#Create a separate dataset for 2018 and 2023 data
SPL_1823 <- SPL %>%
  filter(study_id=="2018_Seattle_Citywide" | study_id =="2023_Seattle_Citywide")
```

# Create neighborhood column
```{r}
# First match observation site (location_id) with the neighborhoods they're in (location_neighborhood)
SPL_sites <- SPL_sites %>% select(location_id, location_neighborhood) # just select the location_neighborhood col that we care about
SPL_1823 <- left_join(SPL_1823, SPL_sites, by = "location_id") # merge the SPL data with the location_neighborhood col

# Check to see that all location_ids have a corresponding location_neighborhood
unique(SPL_1823[is.na(SPL_1823$location_neighborhood), ]$location_id) # missing location_neighborhood for a few

# Fill in NA location_neighborhoods manually
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "BLV3")] <- "Bitter Lake Village"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "PIK6")] <- "Pike/Pine"
SPL_1823$location_neighborhood[which(SPL_1823$location_id %in% c("PIO20", "PIO21"))] <- "Pioneer Square"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "CAP5")] <- "Capitol Hill"
SPL_1823$location_neighborhood[which(SPL_1823$location_id %in% c("COM5", "COM6", "COM7", "COM8"))] <- "Commercial Core"
SPL_1823$location_neighborhood[which(SPL_1823$location_id == "FHT5")] <- "First Hill / 12th Ave"

# Rename neighborhoods to match the names in the geospatial data (S_HOOD)
SPL_1823 <- SPL_1823 %>%
  mutate(S_HOOD = recode(location_neighborhood,
                          "23rd & Union - Jackson" = "Pinehurst", #change this
                          "Capitol Hill" = "Broadway",
                          "Bitter Lake Village" = "Bitter Lake",
                          "Magnolia" = "Carleton Park",
                          "Upper Queen Anne" = "North Queen Anne",
                          "Pike/Pine" = "Pike-Market",
                          "Greenwood - Phinney Ridge" = "Greenwood",
                          "Westwood - Highland Park"= "Highland Park",
                          "Othello" = "Columbia City",                
                          "Madison Miller" = "Stevens",
                          "Uptown" = "Lower Queen Anne",
                          "First Hill / 12th Ave" = "Broadview",
                          "West Seattle Junction" = "Gatewood",
                          "Commercial Core" = "Central Business District",
                          "Mt Baker"= "Mount Baker",              
                          "Chinatown / ID" = "International District",          
                          "Northgate" = "Maple Leaf",              
                          "Lake City" = "Victory Heights"))
```

# Subset geo data for 2018 and 2023 locations
```{r}
# First check to see if we have shapefiles for all location_ids
spl_18_locids <- unique(SPL_1823$location_id)
geo_locids <- unique(geo_SPL_locationids$location_id)
spl_18_locids[which(!spl_18_locids %in% geo_locids)] # missing shapefiles for these: BLT5, PIK5, BLV3, PIK6, HPT1, PIO21, PIO20, HPT2, CAP5, COM5, COM6, COM7, COM8, FHT5

# Identify which study the missing shapefiles are from
SPL_1823 %>% select(study_id, location_id) %>% filter(location_id %in% c("BLT5", "PIK5", "BLV3", "PIK6", "HPT1", "PIO21", "PIO20", "HPT2", "CAP5", "COM5", "COM6", "COM7", "COM8", "FHT5")) %>% table() # missing shapefiles are all from 2023 survey

# Filter geodata for just the location_ids in our 2018/2023 data
geo_SPL_1823_locationids <- geo_SPL_locationids %>% filter(location_id %in% spl_18_locids) # should have 108 location_ids

# Extract lat/long coordinates from centroids
geo_SPL_1823_locationids <- geo_SPL_1823_locationids %>%
  mutate(longitude=st_coordinates(st_centroid(geometry))[,1], #Extract the longitude 
         latitude=st_coordinates(st_centroid(geometry))[,2])   #Extract the latitude

# test plot
ggplot(geo_SPL_1823_locationids) +
  geom_sf()
```

# Export data
```{r}
write.csv(SPL_1823, here("data", "SPL_1823.csv"))
sf::st_write(geo_SPL_1823_locationids,here("data", "SPL_1823_location.geojson"))
```
